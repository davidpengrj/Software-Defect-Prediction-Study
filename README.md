
# Software Defect Prediction Framework & Empirical Study on Borderline Oversampling

![Python](https://img.shields.io/badge/Python-3.10-blue.svg)
![Scikit-Learn](https://img.shields.io/badge/Library-Scikit--Learn-orange.svg)
![Imbalanced-Learn](https://img.shields.io/badge/Technique-Borderline--SMOTE-green)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)

## üìñ Project Overview

This project establishes a comprehensive empirical research framework for **Software Defect Prediction (SDP)**. It addresses the critical challenge of class imbalance in software engineering datasets by integrating advanced oversampling techniques.

The project consists of two core components:

1. **Foundation Framework**: Based on the paper *Software defect prediction: future directions and challenges*, implemented to explore the general SDP process.
2. **Improved Framework**: Addressing the pervasive **Class Imbalance (Skewed Distribution)** issue. It introduces the **Borderline-SMOTE** technique, inspired by the paper *"Estimating Uncertainty in Line-Level Defect Prediction via Perceptual Borderline Oversampling"*.

---

## üåü Core Highlights

### üß† Algorithm Diversity

This project implements over **15 algorithms**, spanning from classic machine learning to deep learning architectures:

| Category                                   | Algorithms & Description                                                                                                           |
| :----------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------- |
| **A. Ensemble Methods**<br>*(Most Robust)* | ‚Ä¢ **Random Forest**<br>‚Ä¢ **Extra Trees**<br>‚Ä¢ **Gradient Boosting**<br>‚Ä¢ **AdaBoost**<br>‚Ä¢ **XGBoost**<br>‚Ä¢ **Bagging Classifier** |
| **B. Deep Learning**                       | ‚Ä¢ **MLPClassifier**<br>‚Ä¢ **Deep MLP**                                                                                              |
| **C. Linear & Statistical**                | ‚Ä¢ **Logistic Regression**<br>‚Ä¢ **SGD Classifier**<br>‚Ä¢ **Passive Aggressive**<br>‚Ä¢ **LDA & QDA**                                   |
| **D. Classic Algorithms**                  | ‚Ä¢ **SVC**<br>‚Ä¢ **GaussianNB**<br>‚Ä¢ **KNeighbors**<br>‚Ä¢ **Decision Tree**                                                           |

---

## üìä Robust Evaluation

> **‚ö†Ô∏è The Accuracy Trap**
> Accuracy is abandoned since it is misleading in imbalanced settings.

Primary evaluation metrics:

* **MCC**
* **ROC-AUC**
* **F1-Score**

---

## üí° Methodological Innovation

The core of the improved method is **Borderline-SMOTE**, which generates synthetic data only for *‚Äúdanger‚Äù* minority samples‚Äîthose lying near class boundaries.

This enhances the model‚Äôs capability to detect actual defects.

---

## üìÇ Datasets

Evaluated on 5 PROMISE datasets: `cm1`, `jm1`, `kc1`, `kc2`, `pc1`.

---

# üèóÔ∏è Repository Structure

```
.
‚îú‚îÄ‚îÄ SDP_Paper_Inspired_Oversampling.py         # Proposed method (Borderline-SMOTE)
‚îú‚îÄ‚îÄ SDP_Paper_Inspired_Baseline_NoSampling.py  # Baseline without sampling
‚îú‚îÄ‚îÄ Advanced_SDP_Framework.py                  # Foundation cost-sensitive framework
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

### Detailed Description

1. **`SDP_Paper_Inspired_Oversampling.py` (Proposed Method)**

   * Applies **Borderline-SMOTE**
   * Enhances MCC and Recall
   * Uses 9 advanced models

2. **`SDP_Paper_Inspired_Baseline_NoSampling.py` (Baseline)**

   * No oversampling
   * Demonstrates the **Accuracy Trap**

3. **`Advanced_SDP_Framework.py` (Foundation)**

   * Uses cost-sensitive learning (`class_weight='balanced'`)
   * Supports GridSearchCV

---

# ‚öôÔ∏è Methodology

### 3.1 Data Preprocessing

* Handles missing values (`?`) using median imputation
* Stratified train-test split

### 3.2 Borderline Oversampling

Borderline-SMOTE classifies minority instances into:

* üü¢ Safe
* üî¥ Noise
* ‚ö†Ô∏è Danger (**oversampled**)

> The system generates synthetic samples only for **danger** points.

---

# üöÄ Quick Start

### Prerequisites

Use Anaconda:

```bash
# 1. Create environment
conda create -n sdp_env python=3.10
conda activate sdp_env

# 2. Install dependencies
pip install -r requirements.txt
```

---

# ‚ñ∂Ô∏è Running the Experiments

## **1. Run the Improved Method (View Performance Gains)**

```bash
python SDP_Paper_Inspired_Oversampling.py
```

---

## **2. Run the Baseline (View Original Performance)**

```bash
python SDP_Paper_Inspired_Baseline_NoSampling.py
```

---

## **3. Run the Foundation Framework**

```bash
python Advanced_SDP_Framework.py
```

---

# üß™ Key Findings

Through comparative experiments, we reached the following conclusions:

### üö´ **The Accuracy Trap**

Baseline models (e.g., AdaBoost) reached **90% Accuracy** on `cm1`, but MCC was **0.0**, meaning:

* The classifier predicted **all samples as "No Defect"**
* It learned *nothing*

### ‚úÖ **Effectiveness of Oversampling**

After introducing **Borderline-SMOTE**:

* Accuracy dropped slightly (back to reality)
* **MCC rose significantly** (e.g., 0.00 ‚Üí 0.37+)
  This proves the model began truly identifying defects.

### üèÖ **Model Recommendations**

* **ExtraTrees** & **GradientBoosting** are most robust with oversampling
* **Deep MLP** captures non-linear patterns effectively

---

# üìö References

1. Verma, A., & Sharma, A. (2024). *Software defect prediction: future directions and challenges.* Empirical Software Engineering, 29(6), 143.
2. Chen, W., et al. *Estimating Uncertainty in Line-Level Defect Prediction via Perceptual Borderline Oversampling.*


