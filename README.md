# Software Defect Prediction Framework & Empirical Study on Borderline Oversampling

![Python](https://img.shields.io/badge/Python-3.10-blue.svg)
![Scikit-Learn](https://img.shields.io/badge/Library-Scikit--Learn-orange.svg)
![Imbalanced-Learn](https://img.shields.io/badge/Technique-Borderline--SMOTE-green)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)

## üìñ Project Overview

This project establishes a comprehensive empirical research framework for **Software Defect Prediction (SDP)**. It addresses the critical challenge of class imbalance in software engineering datasets by integrating advanced oversampling techniques.

The project consists of two core components:
1.  **Foundation Framework**: Based on the paper *Software defect prediction: future directions and challenges*, implemented to explore the general SDP process.
2.  **Improved Framework**: Addressing the pervasive **Class Imbalance (Skewed Distribution)** issue. It introduces the **Borderline-SMOTE** technique, inspired by the paper *"Estimating Uncertainty in Line-Level Defect Prediction via Perceptual Borderline Oversampling"*.

---

## üåü Core Highlights

### üß† Algorithm Diversity
This project implements over **15 algorithms**, spanning from classic machine learning to deep learning architectures:

| Category | Algorithms & Description |
| :--- | :--- |
| **A. Ensemble Methods**<br>*(Most Robust)* | ‚Ä¢ **Random Forest**: Bagging-based, reduces variance.<br>‚Ä¢ **Extra Trees**: Extremely Randomized Trees, superior variance control.<br>‚Ä¢ **Gradient Boosting**: Iteratively corrects errors of predecessors.<br>‚Ä¢ **AdaBoost**: Focuses on misclassified samples.<br>‚Ä¢ **XGBoost**: Optimized distributed gradient boosting.<br>‚Ä¢ **Bagging Classifier**: General bootstrap aggregating. |
| **B. Deep Learning** | ‚Ä¢ **MLPClassifier**: Basic structure.<br>‚Ä¢ **Deep MLP**: 3-layer structure (100-50-25) to simulate deep feature extraction. |
| **C. Linear & Statistical** | ‚Ä¢ **Logistic Regression**: Classic linear baseline.<br>‚Ä¢ **SGD Classifier**: Stochastic Gradient Descent for large-scale data.<br>‚Ä¢ **Passive Aggressive**: Online learning for data streams.<br>‚Ä¢ **LDA & QDA**: Discriminant Analysis. |
| **D. Classic Algorithms** | ‚Ä¢ **SVC**: Support Vector Machine.<br>‚Ä¢ **GaussianNB**: Naive Bayes.<br>‚Ä¢ **KNeighbors**: KNN.<br>‚Ä¢ **Decision Tree**: CART. |

### üìä Robust Evaluation
> **‚ö†Ô∏è The Accuracy Trap**: We explicitly abandon the misleading "Accuracy" metric, which is prone to bias in imbalanced datasets.

Instead, we evaluate models using:
* **MCC (Matthews Correlation Coefficient)**: The most reliable metric for imbalanced binary classification.
* **ROC-AUC**: Area Under the Receiver Operating Characteristic Curve.
* **F1-Score**: Harmonic mean of Precision and Recall.

### üí° Methodological Innovation
Introduction of **Borderline Oversampling** to actively handle data skewness. Unlike random oversampling, this method synthesizes new data **only for difficult samples near the decision boundary**, enhancing the model's sensitivity to defects.

### üìÇ Datasets
Validated at the function/module level using 5 classic **PROMISE datasets**: `cm1`, `jm1`, `kc1`, `kc2`, `pc1`.

---

## üèóÔ∏è Repository Structure

This repository is designed for **A/B Testing** across three distinct stages:

```text
.
‚îú‚îÄ‚îÄ SDP_Paper_Inspired_Oversampling.py      # [Proposed Method] Core experiment with Borderline-SMOTE
‚îú‚îÄ‚îÄ SDP_Paper_Inspired_Baseline_NoSampling.py # [Baseline] Control group with no sampling
‚îú‚îÄ‚îÄ Advanced_SDP_Framework.py               # [Foundation] Cost-Sensitive learning framework
‚îú‚îÄ‚îÄ requirements.txt                        # Project dependencies
‚îî‚îÄ‚îÄ README.md
